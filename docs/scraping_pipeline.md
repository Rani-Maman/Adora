# Adora â€” Scraping Pipeline & System Architecture

> Last updated: February 2026

## Overview

Adora is an Israeli dropship/scam detection system. It scrapes Facebook/Meta Ad Library for Hebrew keyword ads, analyzes advertiser product sites for dropshipping indicators using Playwright + Gemini AI, and serves risk scores to a Chrome extension in real-time.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NIGHTLY PIPELINE                        â”‚
â”‚                                                             â”‚
â”‚   00:01  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â–ºâ”‚ Keyword Job 1â”‚  (mivtsa / ××‘×¦×¢)                  â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚   01:00  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â–ºâ”‚ Keyword Job 2â”‚  (mugbal / ××•×’×‘×œ)                  â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚   02:00  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â–ºâ”‚ Keyword Job 3â”‚  (hanaha / ×”× ×—×ª)                   â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚   03:00  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â–ºâ”‚ Keyword Job 4â”‚  (shaot / ×©×¢×•×ª)                    â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚   04:00  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â–ºâ”‚ Keyword Job 5â”‚  (achshav / ×¢×›×©×™×•)                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚   05:00  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â–ºâ”‚ Nightly Email Summaryâ”‚  â†’ Gmail                  â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONTINUOUS PIPELINE                         â”‚
â”‚                                                             â”‚
â”‚  Every   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  10 min  â”‚ Batch Analyze (20) â”‚â”€â”€â”€â”€â–ºâ”‚ risk_dbâ”‚             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Playwright+Gemini  â”‚     â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                   â”‚
â”‚                                         â–¼                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚          â”‚ Chrome Ext  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  FastAPI /checkâ”‚         â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Data Collection â€” Meta Ad Library Scraper

### Architecture

The scraper uses **Playwright browser automation** to navigate the Meta Ad Library, search Hebrew keywords targeting Israel, and extract ad data (advertiser name, page URL, ad body text, link URLs).

Each keyword runs as an independent cron job, staggered 1 hour apart to avoid rate limits.

### Components

| File | Role |
|------|------|
| `backend/scripts/daily_meta_scrape.py` | Core Playwright scraper (~1050 lines) |
| `backend/scripts/run_meta_keyword_job.sh` | Bash wrapper with locking, cleanup, timeouts |
| `backend/scripts/configs/meta_keywords/*.json` | Per-keyword config files (search URL, params) |

### Scraper Flow

```
run_meta_keyword_job.sh
  â”œâ”€â”€ flock (prevent concurrent runs)
  â”œâ”€â”€ cleanup_playwright_orphans() (kill stale Chrome)
  â”œâ”€â”€ timeout --signal=TERM $HARD_TIMEOUT
  â””â”€â”€ python3 daily_meta_scrape.py --config $CONFIG
        â”œâ”€â”€ Launch Playwright (chromium, headless)
        â”œâ”€â”€ Load Meta session cookies (storage state)
        â”œâ”€â”€ For each ad library search link:
        â”‚     â”œâ”€â”€ Navigate to Meta Ad Library URL
        â”‚     â”œâ”€â”€ Scroll & collect ads (max 700 scrolls, 45 idle rounds)
        â”‚     â”œâ”€â”€ Extract: advertiser_name, page_url, ad_body, external_links
        â”‚     â””â”€â”€ Filter: remove social URLs (fb, ig, wa, messenger)
        â”œâ”€â”€ Dedup by SHA1(date + keyword + normalized_name)
        â”œâ”€â”€ Insert into meta_ads_daily table
        â”œâ”€â”€ Also insert into legacy advertisers + ads_with_urls tables
        â””â”€â”€ Save JSON output + log files
```

### Deduplication

- **Key**: `SHA1(scrape_date + keyword + normalized_advertiser_name)`
- Normalized = lowercase â†’ strip non-alphanumeric â†’ collapse whitespace
- `ON CONFLICT DO NOTHING` â€” duplicates silently skipped

### URL Filtering

External URLs are extracted from ad text/links. The following are excluded:
- Social platforms: `facebook.com`, `instagram.com`, `whatsapp.com`, `wa.me`, `messenger.com`
- Marketplace/internal: `marketplace.facebook.com`
- URL shorteners: `bit.ly`, `tinyurl.com`, etc.

### Configuration (Environment Variables)

| Variable | Default | Description |
|----------|---------|-------------|
| `META_DAILY_STORAGE_STATE` | â€” | Path to Playwright storage state (Meta cookies) |
| `META_DAILY_OUTPUT_DIR` | `./output` | JSON output directory |
| `META_DAILY_LOG_DIR` | `./logs` | Log directory |
| `META_DAILY_HARD_TIMEOUT` | `2100` | Max runtime per keyword (seconds) |
| `META_DAILY_MAX_SCROLLS` | `700` | Max scroll attempts per search link |
| `META_DAILY_IDLE_ROUNDS` | `45` | Stop scrolling after N rounds with no new ads |

---

## 2. Analysis Pipeline â€” Batch Scoring

### Architecture

Every 10 minutes, `batch_analyze_ads.py` picks up **20 unscored ads** from `ads_with_urls` and runs them through a two-stage analysis:

1. **Playwright Site Scrape** â€” Visit the advertiser's product URL, extract structured data
2. **Gemini 2.0 Flash AI Scoring** â€” Send site data to Google's Gemini API for fraud analysis

### Components

| File | Role |
|------|------|
| `backend/batch_analyze_ads.py` | Main batch processor |
| `backend/app/analysis/gemini_scorer.py` | Gemini API scorer (also used by FastAPI) |
| `backend/app/scraping/site_scraper.py` | Playwright site data extractor |

### Scoring Flow

```
batch_analyze_ads.py (cron: */10)
  â”œâ”€â”€ SELECT 20 rows FROM ads_with_urls WHERE analysis_score IS NULL
  â”œâ”€â”€ Launch single Playwright browser (reused for batch)
  â””â”€â”€ For each ad:
        â”œâ”€â”€ Navigate to product URL
        â”œâ”€â”€ Extract SiteData:
        â”‚     title, product_name, price, shipping_time,
        â”‚     business_id (×—.×¤.), countdown_timer, scarcity_widgets,
        â”‚     whatsapp_only_contact, page_text (4000 chars)
        â”œâ”€â”€ Send to Gemini 2.0 Flash with Israeli fraud detection prompt
        â”œâ”€â”€ Parse JSON response: {score, is_risky, category, reason, evidence}
        â”œâ”€â”€ UPDATE ads_with_urls SET analysis_score = $score
        â””â”€â”€ If is_risky: UPSERT INTO risk_db (domain, score, evidence)
```

### Score Ranges

| Score | Meaning |
|-------|---------|
| 0.0 â€“ 0.2 | Legitimate business |
| 0.3 â€“ 0.5 | Uncertain / needs review |
| 0.6 â€“ 1.0 | Likely dropship / scam |
| -1 | Scrape failure (won't be retried) |

### Gemini Prompt Design

The Gemini prompt is tuned for Israeli e-commerce fraud:
- Distinguishes legitimate Israeli businesses, courses, services from dropship gadgets
- Considers: Hebrew business registration (×—.×¤.), countdown timers, scarcity widgets, WhatsApp-only contact, unrealistic shipping times, generic product descriptions
- Returns structured JSON with confidence level

### Rate Limiting

- 2-second delay between Gemini API calls
- 3 retries with exponential backoff for 429/RESOURCE_EXHAUSTED errors
- 20 ads per 10-minute window = ~120 ads/hour max throughput

---

## 3. Reporting

### Nightly Combined Email (`nightly_scrape_summary.py`)

Runs at **05:00** daily, after all keyword scraping jobs complete. Sends a single email combining results from all 5 keywords.

**Report format:**
```
Facebook Ads Scrape Summary - February 08, 2026
â° Runtime: 00:01:02 - 04:35:50 (4h 34m)

ğŸ“Š Results:
Total Ads Found: 450
New Advertisers Added: 120
Duplicates Skipped: 330

By Keyword:
âœ…ğŸŸ¢ 150 - ××‘×¦×¢ ads, 120 new, 30 duplicates
âœ…ğŸŸ¢ 100 - ××•×’×‘×œ ads, 80 new, 20 duplicates
âœ…ğŸŸ¢ 80 - ×”× ×—×ª ads, 60 new, 20 duplicates
âœ…ğŸŸ¢ 70 - ×©×¢×•×ª ads, 50 new, 20 duplicates
âœ…ğŸŸ¢ 50 - ×¢×›×©×™×• ads, 40 new, 10 duplicates

ğŸ“ Database:
- All Advertisers: ~9500 total (added 120 today)
- Ads with Valid URLs: 4800 total (added 180 today)
```

### Daily Analysis Report (`daily_report.py`)

Runs at **00:01** daily. Reports on the *analysis* pipeline (not scraping):
- Ads tested yesterday, risky found, safe cleared
- Scrape errors (score = -1), remaining backlog
- Sent via email + appended to log

---

## 4. Database Schema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  meta_ads_daily   â”‚    â”‚   advertisers     â”‚    â”‚ ads_with_urlsâ”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)          â”‚    â”‚ id (PK)          â”‚    â”‚ id (PK)      â”‚
â”‚ advertiser_name  â”‚    â”‚ advertiser_name  â”‚    â”‚ ad_url       â”‚
â”‚ page_url         â”‚    â”‚ page_url         â”‚    â”‚ advertiser   â”‚
â”‚ ad_body          â”‚    â”‚ keyword          â”‚    â”‚ keyword      â”‚
â”‚ external_links   â”‚    â”‚ scraped_at       â”‚    â”‚ scraped_at   â”‚
â”‚ source_keyword   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚analysis_scoreâ”‚
â”‚ scraped_at       â”‚                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ dedup_key (UNQ)  â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚ batch_analyze
                                                       â”‚ (score â‰¥ 0.6)
                                                       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ dropship_analysisâ”‚    â”‚   risk_db    â”‚
                         â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                         â”‚ analysis details â”‚    â”‚ base_url(UNQ)â”‚
                         â”‚ red_flags        â”‚    â”‚ risk_score   â”‚
                         â”‚ aliexpress match â”‚    â”‚ evidence[]   â”‚
                         â”‚ scoring          â”‚    â”‚ advertiser   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ first_seen   â”‚
                                                 â”‚ last_updated â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â–²
                                                       â”‚ /check/?url=
                                                 â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                                 â”‚ Chrome Ext â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Table Purposes

| Table | Purpose | Write Source |
|-------|---------|--------------|
| `meta_ads_daily` | Per-day deduped ads from Playwright scraper | `daily_meta_scrape.py` |
| `advertisers` | All scraped advertisers (legacy + new) | `daily_meta_scrape.py` |
| `ads_with_urls` | Filtered subset with valid external URLs | `daily_meta_scrape.py` |
| `dropship_analysis` | Detailed analysis results | `batch_analyze_ads.py` |
| `risk_db` | Final risk DB â€” only risky sites, queried by extension | `batch_analyze_ads.py` |

---

## 5. Chrome Extension

### Architecture

Manifest V3 Chrome extension with a 3-tier checking system:

```
User navigates to URL
  â”œâ”€â”€ Tier 1: Local whitelist (22k+ safe domains) â†’ instant âœ…
  â”œâ”€â”€ Tier 2: Persistent cache (24h TTL, 1000 entries) â†’ instant âœ…/âš ï¸
  â””â”€â”€ Tier 3: API call â†’ GET /check/?url=X
        â””â”€â”€ FastAPI queries risk_db
              â””â”€â”€ Returns {risky, score, evidence} or {risky: false}
```

### Badge Behavior
- **No badge**: Site not in risk_db or whitelisted
- **Red "!"**: Risk score â‰¥ 0.6 â€” popup shows warning with evidence

### Key Files

| File | Role |
|------|------|
| `extension/public/background.js` | Service worker â€” auto-checks on tab navigation |
| `extension/public/config.js` | API base URL configuration |
| `extension/src/App.jsx` | React popup UI |
| `extension/public/manifest.json` | Chrome Manifest V3 |

---

## 6. FastAPI Backend

### Endpoints

| Method | Path | Purpose |
|--------|------|---------|
| `GET` | `/` | Health check |
| `GET` | `/health` | Detailed health info |
| `GET` | `/check/?url=X` | Lightweight risk_db lookup (extension uses this) |
| `POST` | `/analyze/` | On-demand deep analysis (Playwright + Gemini) |
| `GET` | `/whitelist/domains` | Full whitelist |
| `GET` | `/whitelist/check/{domain}` | Single domain whitelist check |

### Middleware
- CORS (all origins â€” configured for extension access)
- Request logging with timing, client IP, user agent

---

## 7. Cron Schedule (VM)

| Time | Job | Description |
|------|-----|-------------|
| `00:01` | `daily_report.py` | Yesterday's analysis summary email |
| `00:01` | `01_mivtsa.json` | Scrape keyword: ××‘×¦×¢ |
| `01:00` | `02_mugbal.json` | Scrape keyword: ××•×’×‘×œ |
| `02:00` | `03_hanaha.json` | Scrape keyword: ×”× ×—×ª |
| `03:00` | `04_shaot.json` | Scrape keyword: ×©×¢×•×ª |
| `04:00` | `05_achshav.json` | Scrape keyword: ×¢×›×©×™×• |
| `05:00` | `nightly_scrape_summary.py` | Combined scrape results email |
| `*/10` | `batch_analyze_ads.py` | Analyze 20 unscored ads |

---

## 8. Infrastructure

- **VM**: Oracle Cloud (Ubuntu 22.04)
- **Database**: PostgreSQL 14 (localhost)
- **Python**: 3.10 (system)
- **Browser**: Playwright Chromium (headless)
- **AI**: Google Gemini 2.0 Flash
- **Email**: Gmail SMTP (App Password)
- **Extension**: Chrome extension served locally (dev mode) or via Chrome Web Store
- **API Tunnel**: Cloudflare Quick Tunnel (development) or direct IP

---

## 9. End-to-End Data Flow

```
1. SCRAPE (nightly, 5 keywords, staggered hourly)
   Meta Ad Library â†’ Playwright â†’ meta_ads_daily + advertisers + ads_with_urls

2. ANALYZE (every 10 min, batch of 20)
   ads_with_urls (unscored) â†’ Playwright site scrape â†’ Gemini AI â†’ risk_db

3. SERVE (real-time)
   Chrome Extension â†’ FastAPI /check â†’ risk_db â†’ badge + popup warning

4. REPORT (daily)
   05:00 â†’ nightly_scrape_summary.py â†’ combined email
   00:01 â†’ daily_report.py â†’ analysis stats email
```
